\chapter{Source Code Example}
%\label{chapter:title}

{\normalfont\texttt{listings}}. An example can be found below. Files can be added using \\ {\normalfont\texttt{\textbackslash lstinputlisting[language=<language>]\{<filename>\}}}

\begin{lstlisting}[language=Python]
import jax
import jax.numpy as jnp
import optax
from jax import grad, jit, vmap
from jax.scipy.special import logsumexp
from typing import Callable, Tuple

# Generate a random adjacency matrix for a graph
def generate_random_graph(num_nodes: int, edge_prob: float) -> jnp.ndarray:
    """Generates a random adjacency matrix for a graph with given probability of edge existence."""
    rng = jax.random.PRNGKey(0)
    adjacency_matrix = jax.random.bernoulli(rng, p=edge_prob, shape=(num_nodes, num_nodes))
    adjacency_matrix = jnp.triu(adjacency_matrix, 1)  # Upper triangular to avoid self-loops
    adjacency_matrix = adjacency_matrix + adjacency_matrix.T  # Symmetrize
    return adjacency_matrix

# Define the graph coloring loss function
def graph_coloring_loss(colors: jnp.ndarray, adjacency_matrix: jnp.ndarray) -> jnp.ndarray:
    """Computes the loss function for the graph coloring problem."""
    num_nodes = adjacency_matrix.shape[0]
    color_matrix = jnp.expand_dims(colors, 0) == jnp.expand_dims(colors, 1)
    adjacency_loss = jnp.sum(adjacency_matrix * color_matrix)
    return adjacency_loss

# Define the optimization step
def update(params: jnp.ndarray, opt_state: optax.OptState, grads: jnp.ndarray, optimizer: optax.GradientTransformation) -> Tuple[jnp.ndarray, optax.OptState]:
    """Updates parameters using Optax optimizer."""
    updates, opt_state = optimizer.update(grads, opt_state, params)
    return optax.apply_updates(params, updates), opt_state

# Main function to solve the graph coloring problem
def graph_coloring(num_nodes: int, edge_prob: float, num_colors: int, learning_rate: float, num_steps: int):
    adjacency_matrix = generate_random_graph(num_nodes, edge_prob)
    
    # Initialize color assignments randomly
    init_colors = jax.random.randint(jax.random.PRNGKey(1), shape=(num_nodes,), minval=0, maxval=num_colors)
    
    # Define the optimizer
    optimizer = optax.adam(learning_rate)
    
    # Define the loss function
    loss_fn = lambda colors: graph_coloring_loss(colors, adjacency_matrix)
    
    # Initialize optimizer state
    opt_state = optimizer.init(init_colors)
    
    # Gradient function
    grad_fn = jit(grad(loss_fn))
    
    # Optimization loop
    colors = init_colors
    for step in range(num_steps):
        grads = grad_fn(colors)
        colors, opt_state = update(colors, opt_state, grads, optimizer)
        
        # Print progress
        if step % 100 == 0:
            current_loss = loss_fn(colors)
            print(f"Step {step}, Loss: {current_loss:.2f}")

    return colors

# Parameters
num_nodes = 10
edge_prob = 0.3
num_colors = 3
learning_rate = 0.01
num_steps = 1000

# Run the graph coloring optimization
optimized_colors = graph_coloring(num_nodes, edge_prob, num_colors, learning_rate, num_steps)
print("Optimized Colors:", optimized_colors)
\end{lstlisting}